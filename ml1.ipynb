{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "In Machine Learning (ML), a parameter refers to an internal variable of a model that is learned from the training data.\n",
        "\n",
        "They are the parts of the model that define how the input data is transformed into output predictions.\n",
        "\n",
        "Parameters are adjusted during the training process using optimization algorithms (like gradient descent) to minimize the error between predicted and actual outputs.\n",
        "\n",
        "Examples:\n",
        "\n",
        "In Linear Regression, the slope (weights) and intercept (bias) are parameters.\n",
        "\n",
        "In a Neural Network, the weights and biases of the connections between neurons are parameters."
      ],
      "metadata": {
        "id": "qGRSqHDh4p4J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a54e5f74"
      },
      "source": [
        "2. What is correlation?\n",
        "\n",
        "Correlation is a statistical measure that expresses the extent to which two variables are linearly related. It indicates how closely two variables move together. A positive correlation means that as one variable increases, the other variable also tends to increase. A negative correlation means that as one variable increases, the other variable tends to decrease. The correlation coefficient is a value between -1 and 1 that quantifies the strength and direction of the linear relationship. A value of 1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\n",
        "\n",
        "3. What does negative correlation mean?\n",
        "\n",
        "Negative correlation means that there is an inverse relationship between two variables. As the value of one variable increases, the value of the other variable tends to decrease. For example, there might be a negative correlation between the number of hours a student spends watching TV and their test scores. This would suggest that students who watch more TV tend to have lower test scores, and students who watch less TV tend to have higher test scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b2cabe0"
      },
      "source": [
        "4.  **Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "    Machine learning is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. The main components typically include:\n",
        "\n",
        "    *   **Data:** The raw information used to train and evaluate the model.\n",
        "    *   **Model:** The algorithm or structure that learns from the data.\n",
        "    *   **Parameters:** The internal variables of the model learned from the training data.\n",
        "    *   **Loss function:** A measure of how well the model is performing.\n",
        "    *   **Optimization algorithm:** A method used to adjust the model's parameters to minimize the loss function.\n",
        "\n",
        "5.  **How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        "    The loss value quantifies the error between the model's predictions and the actual target values. A lower loss value indicates that the model's predictions are closer to the actual values, suggesting a better-performing model. During training, the goal is to minimize the loss value.\n",
        "\n",
        "6.  **What are continuous and categorical variables?**\n",
        "\n",
        "    *   **Continuous variables:** Variables that can take on any value within a given range (e.g., height, temperature, time).\n",
        "    *   **Categorical variables:** Variables that can take on a limited number of distinct values, often representing categories or groups (e.g., gender, color, country).\n",
        "\n",
        "7.  **How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "\n",
        "    Machine learning algorithms typically require numerical input. Therefore, categorical variables need to be converted into a numerical format. Common techniques include:\n",
        "\n",
        "    *   **One-Hot Encoding:** Creates new binary columns for each category.\n",
        "    *   **Label Encoding:** Assigns a unique integer to each category.\n",
        "    *   **Ordinal Encoding:** Assigns integers based on the order or rank of the categories (if there is a natural order).\n",
        "\n",
        "8.  **What do you mean by training and testing a dataset?**\n",
        "\n",
        "    *   **Training set:** The portion of the dataset used to train the machine learning model. The model learns the patterns and relationships in the data from this set.\n",
        "    *   **Testing set:** The portion of the dataset used to evaluate the performance of the trained model on unseen data. This helps assess how well the model generalizes to new examples.\n",
        "\n",
        "9.  **What is sklearn.preprocessing?**\n",
        "\n",
        "    `sklearn.preprocessing` is a module in the scikit-learn library that provides a collection of functions and classes to preprocess data before training a machine learning model. This includes techniques for scaling, centering, normalizing, and encoding data.\n",
        "\n",
        "10.  **What is a Test set?**\n",
        "\n",
        "    A test set is a subset of the original data that is held out and not used during the model training process. Its purpose is to provide an unbiased evaluation of the final model's performance on new, unseen data.\n",
        "\n",
        "11.  **How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "    The `train_test_split` function from `sklearn.model_selection` is commonly used to split data into training and testing sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc09f309"
      },
      "source": [
        "12.  **What is causation? Explain the difference between correlation and causation with an example.**\n",
        "\n",
        "    **Causation** means that one event is the direct result of another event. In other words, one variable directly influences or causes a change in another variable.\n",
        "\n",
        "    **Correlation** indicates that two variables are related and tend to change together, but it doesn't necessarily mean that one causes the other.\n",
        "\n",
        "    **Difference:** Correlation shows a relationship, while causation shows a cause-and-effect link.\n",
        "\n",
        "    **Example:** There is a strong correlation between ice cream sales and the number of drowning incidents. As ice cream sales increase, the number of drownings also tends to increase. However, this doesn't mean that eating ice cream causes drowning. The underlying cause for both is likely the weather â€“ hot weather leads to both increased ice cream sales and more people swimming, which in turn can lead to more drownings. This is an example of correlation without causation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca22936b"
      },
      "source": [
        "13.  **What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "    An **optimizer** is an algorithm used to minimize the loss function of a machine learning model during training. It adjusts the model's parameters (weights and biases) iteratively to find the values that result in the lowest possible loss.\n",
        "\n",
        "    **Types of Optimizers:**\n",
        "\n",
        "    *   **Gradient Descent:** A basic optimization algorithm that updates parameters in the opposite direction of the gradient of the loss function. It can be slow for large datasets.\n",
        "        *   *Example:* Imagine a ball rolling down a hill. Gradient descent is like the ball taking steps in the steepest downward direction to reach the bottom (minimum loss).\n",
        "\n",
        "    *   **Stochastic Gradient Descent (SGD):** Updates parameters using the gradient of the loss function computed on a single randomly selected training example at each step. This is faster than standard gradient descent for large datasets but can have noisy updates.\n",
        "        *   *Example:* Instead of looking at the whole hill (dataset) to decide where to step, SGD looks at just one small part (single example) to make a step. This is faster but might not always take the most direct path.\n",
        "\n",
        "    *   **Mini-batch Gradient Descent:** Updates parameters using the gradient of the loss function computed on a small batch of training examples at each step. This is a compromise between standard gradient descent and SGD, offering a balance of speed and stability.\n",
        "        *   *Example:* Instead of one example or the whole hill, mini-batch gradient descent looks at a small group of examples (a mini-batch) to decide where to step.\n",
        "\n",
        "    *   **Adam (Adaptive Moment Estimation):** An adaptive learning rate optimization algorithm that uses estimates of first and second moments of the gradients to adjust the learning rate for each parameter individually. It is widely used and often performs well in practice.\n",
        "        *   *Example:* Adam is like a smart ball rolling down the hill that remembers where it has been and how fast it was going to adjust its steps more effectively.\n",
        "\n",
        "    *   **RMSprop (Root Mean Square Propagation):** Another adaptive learning rate optimization algorithm that divides the learning rate by an exponentially decaying average of squared gradients. It helps to handle vanishing and exploding gradients.\n",
        "        *   *Example:* RMSprop is like a ball that adjusts its speed based on how steep the hill has been recently.\n",
        "\n",
        "    *   **Adagrad (Adaptive Gradient):** An adaptive learning rate optimization algorithm that scales the learning rate for each parameter based on the historical sum of squared gradients. It is well-suited for sparse data but the learning rate can become very small over time.\n",
        "        *   *Example:* Adagrad is like a ball that slows down more and more in directions where the hill has been consistently steep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d18e8a6a"
      },
      "source": [
        "14.  **What is sklearn.linear_model?**\n",
        "\n",
        "    `sklearn.linear_model` is a module within the scikit-learn library that provides a variety of linear models for regression, classification, and other tasks. This includes algorithms like Linear Regression, Logistic Regression, Ridge, Lasso, and Elastic Net."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31e8c616"
      },
      "source": [
        "15.  **What does model.fit() do? What arguments must be given?**\n",
        "\n",
        "    `model.fit()` is a method used to train a machine learning model. It learns the patterns and relationships in the training data by adjusting the model's internal parameters.\n",
        "\n",
        "    The required arguments for `model.fit()` are typically:\n",
        "\n",
        "    *   `X`: The training data features (input variables). This is usually a 2D array or DataFrame where rows represent samples and columns represent features.\n",
        "    *   `y`: The target variable (output variable) for the training data. This is usually a 1D array or Series.\n",
        "\n",
        "    Some models may have additional optional arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b226cc6"
      },
      "source": [
        "16.  **What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "    `model.predict()` is a method used to make predictions on new, unseen data using a trained machine learning model.\n",
        "\n",
        "    The required argument for `model.predict()` is typically:\n",
        "\n",
        "    *   `X`: The data for which you want to make predictions. This is usually a 2D array or DataFrame with the same number of features as the training data used to fit the model.\n",
        "\n",
        "    The method returns the predicted values based on the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd678a5c"
      },
      "source": [
        "17.  **What are continuous and categorical variables?**\n",
        "\n",
        "    This question was answered previously in this notebook. Please refer to the markdown cell with id `0b2cabe0` for the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe57a7ed"
      },
      "source": [
        "18.  **What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        "    **Feature scaling** is a data preprocessing technique used to standardize or normalize the range of independent variables (features) in a dataset.\n",
        "\n",
        "    **How it helps:**\n",
        "\n",
        "    *   **Improves the performance of distance-based algorithms:** Algorithms like K-Nearest Neighbors (KNN) and Support Vector Machines (SVM) are sensitive to the scale of features. Scaling ensures that all features contribute equally to the distance calculations.\n",
        "    *   **Speeds up gradient descent:** Optimization algorithms like gradient descent converge faster when features are on a similar scale.\n",
        "    *   **Helps in regularization:** Techniques like Ridge and Lasso regularization are affected by the scale of features. Scaling ensures that the regularization penalty is applied fairly to all features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6d5099a"
      },
      "source": [
        "19.  **How do we perform scaling in Python?**\n",
        "\n",
        "    You can perform scaling in Python using the `sklearn.preprocessing` module. Common techniques include:\n",
        "\n",
        "    *   **Standardization (StandardScaler):** Scales features to have zero mean and unit variance.\n",
        "    *   **Normalization (MinMaxScaler):** Scales features to a specific range, typically between 0 and 1.\n",
        "\n",
        "    Here's an example using `StandardScaler`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "969a2454"
      },
      "source": [
        "20. **What is sklearn.preprocessing?**\n",
        "\n",
        "    `sklearn.preprocessing` is a module in the scikit-learn library that provides a collection of functions and classes to preprocess data before training a machine learning model. This includes techniques for scaling, centering, normalizing, and encoding data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49d3abc7"
      },
      "source": [
        "21. **How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "    The `train_test_split` function from `sklearn.model_selection` is commonly used to split data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ca65e80",
        "outputId": "89515085-e932-4b08-8297-67ec70658c58"
      },
      "source": [
        "    from sklearn.model_selection import train_test_split\n",
        "    import numpy as np\n",
        "\n",
        "    # Sample data (features and target)\n",
        "    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "    y = np.array([0, 1, 0, 1, 0])\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"X_train:\", X_train)\n",
        "    print(\"X_test:\", X_test)\n",
        "    print(\"y_train:\", y_train)\n",
        "    print(\"y_test:\", y_test)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: [[ 9 10]\n",
            " [ 5  6]\n",
            " [ 1  2]\n",
            " [ 7  8]]\n",
            "X_test: [[3 4]]\n",
            "y_train: [0 0 0 1]\n",
            "y_test: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a061a4f0"
      },
      "source": [
        "22. **Explain data encoding?**\n",
        "\n",
        "    Data encoding is the process of converting categorical data into a numerical format that can be understood and processed by machine learning algorithms. Since most algorithms require numerical input, encoding is a crucial step in preparing data with categorical features.\n",
        "\n",
        "    Common data encoding techniques include:\n",
        "\n",
        "    *   **One-Hot Encoding:** Creates new binary columns for each unique category in a categorical feature. If a data point belongs to a specific category, the corresponding binary column for that category will have a value of 1, and all other binary columns for that feature will be 0. This is suitable for nominal categorical variables where there is no inherent order.\n",
        "\n",
        "    *   **Label Encoding:** Assigns a unique integer to each unique category in a categorical feature. This is suitable for ordinal categorical variables where there is a natural order or ranking among the categories. However, it can introduce unintended ordinal relationships if used for nominal variables.\n",
        "\n",
        "    *   **Ordinal Encoding:** Similar to Label Encoding, but allows you to specify the order of the categories. This is useful when the ordinal relationship is explicitly known."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe0b70f1",
        "outputId": "ac70e1e4-8df8-49c1-d8e8-892e49c0ccb4"
      },
      "source": [
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import numpy as np\n",
        "\n",
        "    # Sample data\n",
        "    data = np.array([[1, 100], [2, 200], [3, 300]])\n",
        "\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler to the data and transform it\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "    print(scaled_data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e71e8b1"
      },
      "source": [
        "23.  **How can you find correlation between variables in Python?**\n",
        "\n",
        "    You can use libraries like pandas to calculate the correlation between variables in a DataFrame. The `.corr()` method on a pandas DataFrame computes the pairwise correlation of columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e529a4f8",
        "outputId": "b6090fbb-4a88-4b7f-d22e-3251c72775d5"
      },
      "source": [
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import numpy as np\n",
        "\n",
        "    # Sample data\n",
        "    data = np.array([[1, 100], [2, 200], [3, 300]])\n",
        "\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler to the data and transform it\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "    print(scaled_data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    }
  ]
}